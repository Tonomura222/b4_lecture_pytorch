{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Netを用いたセグメンテーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込みの準備\n",
    "正解ラベル画像がない学習画像が存在するため、正解のある画像のみのリストを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464 724 725\n"
     ]
    }
   ],
   "source": [
    "train_path = \"./VOCtrainval_11-May-2012/VOCdevkit/VOC2012/ImageSets/Segmentation/train.txt\"\n",
    "val_path = \"./VOCtrainval_11-May-2012/VOCdevkit/VOC2012/ImageSets/Segmentation/val.txt\"\n",
    "\n",
    "def txt_to_pathlist(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "\n",
    "        image_paths = [\"./VOCtrainval_11-May-2012/VOCdevkit/VOC2012/JPEGImages/\"+fn+\".jpg\" for fn in lines]\n",
    "        label_paths = [\"./VOCtrainval_11-May-2012/VOCdevkit/VOC2012/SegmentationClass/\"+fn+\".png\" for fn in lines]\n",
    "    return image_paths, label_paths\n",
    "\n",
    "train_image_paths, train_label_paths = txt_to_pathlist(train_path)\n",
    "v_image_paths, v_label_paths = txt_to_pathlist(val_path)\n",
    "middle = len(v_image_paths)//2\n",
    "val_image_paths = v_image_paths[:middle]\n",
    "val_label_paths = v_label_paths[:middle]\n",
    "test_image_paths = v_image_paths[middle:]\n",
    "test_label_paths = val_label_paths[middle:]\n",
    "\n",
    "print(len(train_image_paths), len(val_image_paths), len(test_image_paths))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "[ 0  1 15 21]\n",
      "torch.Size([128, 128, 22])\n",
      "tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 0, 0,  ..., 0, 0, 0]]])\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(train_image_paths[0])\n",
    "image = image.resize((128,128))\n",
    "image = np.asarray(image).astype(np.float32)\n",
    "image = torch.from_numpy(image).clone()\n",
    "#print(image.size())\n",
    "#print(image)\n",
    "image = image.permute(2,0,1)\n",
    "#print(image.size())\n",
    "#print(image)\n",
    "\n",
    "label = Image.open(train_label_paths[0])\n",
    "label = label.resize((128,128))\n",
    "label = np.asarray(label).astype(np.int64)\n",
    "label = np.where(label == 255, 21, label)\n",
    "label = torch.from_numpy(label)\n",
    "print(label.size())\n",
    "print(label)\n",
    "print(np.unique(label))\n",
    "\n",
    "label = F.one_hot(label, 22)\n",
    "print(label.size())\n",
    "print(label)\n",
    "print(np.unique(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.cls_idx = [i for i in range(22)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        image = image.resize((128,128)) \n",
    "        image = np.asarray(image)\n",
    "        image = torch.from_numpy(image.astype(np.float32)).clone()\n",
    "        image = image.permute(2,0,1) #次元の並び替え\n",
    "\n",
    "        label = Image.open(self.label_paths[idx])\n",
    "        label = label.resize((128,128))\n",
    "        label = np.asarray(label)\n",
    "        label = np.where(label == 255, 21, label) # 境界線マスクを255から21に変更\n",
    "\n",
    "        #onehotベクトル化        \n",
    "        label = torch.from_numpy(label).long().clone()\n",
    "        #label = torch.nn.functional.one_hot(label.long()).to(torch.float32)\n",
    "        #label = label.permute(2,0,1)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = SegmentDataset(train_image_paths, train_label_paths)\n",
    "validation_data = SegmentDataset(val_image_paths, val_label_paths)\n",
    "test_data = SegmentDataset(test_image_paths, test_label_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([16, 3, 128, 128])\n",
      "Shape of y:  torch.Size([16, 128, 128]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# データローダーの作成\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Netの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "class TwoConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size = 3, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.rl = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, kernel_size = 3, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.rl(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.rl(x)\n",
    "        return x\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 2, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "\n",
    "class UNet_2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.TCB1 = TwoConvBlock(3, 64, 64)\n",
    "        self.TCB2 = TwoConvBlock(64, 128, 128)\n",
    "        self.TCB3 = TwoConvBlock(128, 256, 256)\n",
    "        self.TCB4 = TwoConvBlock(256, 512, 512)\n",
    "        self.TCB5 = TwoConvBlock(512, 1024, 1024)\n",
    "        self.TCB6 = TwoConvBlock(1024, 512, 512)\n",
    "        self.TCB7 = TwoConvBlock(512, 256, 256)\n",
    "        self.TCB8 = TwoConvBlock(256, 128, 128)\n",
    "        self.TCB9 = TwoConvBlock(128, 64, 64)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride = 2)\n",
    "        \n",
    "        self.UC1 = UpConv(1024, 512) \n",
    "        self.UC2 = UpConv(512, 256) \n",
    "        self.UC3 = UpConv(256, 128) \n",
    "        self.UC4= UpConv(128, 64)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(64, 22, kernel_size = 1)\n",
    "        self.soft = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.TCB1(x)\n",
    "        x1 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB2(x)\n",
    "        x2 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB3(x)\n",
    "        x3 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB4(x)\n",
    "        x4 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB5(x)\n",
    "\n",
    "        x = self.UC1(x)\n",
    "        x = torch.cat([x4, x], dim = 1)\n",
    "        x = self.TCB6(x)\n",
    "\n",
    "        x = self.UC2(x)\n",
    "        x = torch.cat([x3, x], dim = 1)\n",
    "        x = self.TCB7(x)\n",
    "\n",
    "        x = self.UC3(x)\n",
    "        x = torch.cat([x2, x], dim = 1)\n",
    "        x = self.TCB8(x)\n",
    "\n",
    "        x = self.UC4(x)\n",
    "        x = torch.cat([x1, x], dim = 1)\n",
    "        x = self.TCB9(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        #x = self.soft(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = UNet_2D().to(device)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(pred, tgt):\n",
    "    tgt_mask = F.one_hot(tgt, 22)\n",
    "    print(tgt_mask.size())\n",
    "    pred_mask = torch.argmax(pred, dim=1)\n",
    "    pred_mask = F.one_hot(pred_mask, 22)\n",
    "    print(tgt_mask.size())\n",
    "\n",
    "    #前景のみ\n",
    "    tgt_mask = tgt_mask[:,:,:,1:]\n",
    "    pred_mask = pred_mask[:,:,:,1:]\n",
    "\n",
    "    minibatch_tp = torch.logical_and(tgt_mask,pred_mask).sum()\n",
    "    minibatch_fp = torch.logical_and(torch.logical_not(tgt_mask), pred_mask)\n",
    "    minibatch_fn = torch.logical_and(tgt_mask, torch.logical_not(pred_mask))\n",
    "\n",
    "    Dice = minibatch_tp/(minibatch_tp+0.5*(minibatch_fp+minibatch_fn))\n",
    "    return Dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 損失誤差を計算\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        epoch_loss += loss.item()\n",
    "        # バックプロパゲーション\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    epoch_loss /= size\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            validation_loss += loss_fn(pred, y).item()\n",
    "            #print(pred.shape,y.shape)\n",
    "            _, tags = torch.max(pred, dim=1)\n",
    "            correct = (tags == y).float()\n",
    "            acc = correct.sum() / correct.numel()\n",
    "            #print(acc)\n",
    "            accs.append(acc.cpu().numpy())\n",
    "            #correct += (pred == y).type(torch.float).sum().item()\n",
    "    validation_loss /= size\n",
    "    accuracy = np.mean(accs)\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {validation_loss:>8f} \\n\")\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation損失をもとに最適epochを決定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "torch.Size([16, 128, 128, 22])\n",
      "torch.Size([16, 128, 128, 22])\n",
      "loss: 3.223873  [    0/ 1464]\n",
      "torch.Size([16, 128, 128, 22])\n",
      "torch.Size([16, 128, 128, 22])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     12\u001b[0m     validation_loss \u001b[38;5;241m=\u001b[39m validation(validation_dataloader, model)\n",
      "Cell \u001b[1;32mIn[23], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      9\u001b[0m calc_metrics(pred, y)\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m---> 11\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# バックプロパゲーション\u001b[39;00m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "best_epoch = 0\n",
    "min_loss = 100000\n",
    "os.makedirs(\"./model/\", exist_ok=True)\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    validation_loss = validation(validation_dataloader, model)\n",
    "    validation_losses.append(validation_loss)\n",
    "    if validation_loss < min_loss:\n",
    "        best_epoch = t+1\n",
    "        min_loss = validation_loss\n",
    "    torch.save(model.state_dict(), \"./model/model_\"+str(t+1)+\".pth\")\n",
    "print(best_epoch)\n",
    "\n",
    "#model = UNet_2D()\n",
    "#model.load_state_dict(torch.load(\"model/model_\"+str(best_epoch)+\".pth\"))\n",
    "#test(test_dataloader, model)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失グラフ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
