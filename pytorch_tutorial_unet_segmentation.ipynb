{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Netを用いたセグメンテーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込みの準備\n",
    "正解ラベル画像がない学習画像が存在するため、正解のある画像のみのリストを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2913\n"
     ]
    }
   ],
   "source": [
    "def generate_pathlist(dir_image, dir_label):\n",
    "    paths_image = glob.glob(dir_image + \"/*\")\n",
    "    paths_label = glob.glob(dir_label + \"/*\")\n",
    "\n",
    "    if len(paths_image) == 0 or len(paths_label) == 0:\n",
    "        raise FileNotFoundError(\"Could not load images.\")\n",
    "    # 正解ラベルの拡張子を.pngに書き換えたものがセグメンテーションに使える画像ファイル名\n",
    "    filenames = list(map(lambda path: path.split(os.sep)[-1].split(\".\")[0], paths_label))\n",
    "    paths_image = list(map(lambda filename: dir_image + \"/\" + filename + \".jpg\", filenames))\n",
    "    return paths_image, paths_label\n",
    "\n",
    "dir_image = \"./VOCtrainval_11-May-2012/VOCdevkit/VOC2012/JPEGImages\"\n",
    "dir_label = \"./VOCtrainval_11-May-2012/VOCdevkit/VOC2012/SegmentationClass\"\n",
    "\n",
    "image_paths, label_paths = generate_pathlist(dir_image, dir_label)\n",
    "\n",
    "# 分割してtrain val testのパスのリストを用意する\n",
    "\n",
    "print(len(image_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724 725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_path = \"./VOCtrainval_11-May-2012/VOCdevkit/VOC2012/ImageSets/Segmentation/train.txt\"\n",
    "val_path = \"./VOCtrainval_11-May-2012/VOCdevkit/VOC2012/ImageSets/Segmentation/val.txt\"\n",
    "\n",
    "def txt_to_pathlist(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "\n",
    "        image_paths = [\"./VOCtrainval_11-May-2012/VOCdevkit/VOC2012/JPEGImages/\"+fn+\".jpg\" for fn in lines]\n",
    "        label_paths = [\"./VOCtrainval_11-May-2012/VOCdevkit/VOC2012/SegmentationClass/\"+fn+\".png\" for fn in lines]\n",
    "    return image_paths, label_paths\n",
    "\n",
    "train_image_paths, train_label_paths = txt_to_pathlist(train_path)\n",
    "v_image_paths, v_label_paths = txt_to_pathlist(val_path)\n",
    "middle = len(v_image_paths)//2\n",
    "val_image_paths = v_image_paths[:middle]\n",
    "val_label_paths = v_label_paths[:middle]\n",
    "test_image_paths = v_image_paths[middle:]\n",
    "test_label_paths = val_label_paths[middle:]\n",
    "\n",
    "print(len(val_image_paths), len(test_image_paths))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SegmentDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.cls_idx = [i for i in range(22)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        image = image.resize((128,128)) \n",
    "        image = np.asarray(image)\n",
    "        image = torch.from_numpy(image.astype(np.float32)).clone()\n",
    "        image = image.permute(2,0,1) #次元の並び替え\n",
    "\n",
    "        label = Image.open(self.label_paths[idx])\n",
    "        label = label.resize((128,128))\n",
    "        label = np.asarray(label)\n",
    "        label = np.where(label == 255, 21, label) # 境界線マスクを255から21に変更\n",
    "\n",
    "        #onehotベクトル化        \n",
    "        label = torch.from_numpy(label.astype(np.float32)).long().clone()\n",
    "        #label = torch.nn.functional.one_hot(label.long()).to(torch.float32)\n",
    "        #label = label.permute(2,0,1)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = SegmentDataset(train_image_paths, train_label_paths)\n",
    "validation_data = SegmentDataset(val_image_paths, val_label_paths)\n",
    "test_data = SegmentDataset(test_image_paths, test_label_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([16, 3, 128, 128])\n",
      "Shape of y:  torch.Size([16, 128, 128]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# データローダーの作成\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Netの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "class TwoConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size = 3, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.rl = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, kernel_size = 3, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.rl(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.rl(x)\n",
    "        return x\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 2, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "\n",
    "class UNet_2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.TCB1 = TwoConvBlock(3, 64, 64)\n",
    "        self.TCB2 = TwoConvBlock(64, 128, 128)\n",
    "        self.TCB3 = TwoConvBlock(128, 256, 256)\n",
    "        self.TCB4 = TwoConvBlock(256, 512, 512)\n",
    "        self.TCB5 = TwoConvBlock(512, 1024, 1024)\n",
    "        self.TCB6 = TwoConvBlock(1024, 512, 512)\n",
    "        self.TCB7 = TwoConvBlock(512, 256, 256)\n",
    "        self.TCB8 = TwoConvBlock(256, 128, 128)\n",
    "        self.TCB9 = TwoConvBlock(128, 64, 64)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride = 2)\n",
    "        \n",
    "        self.UC1 = UpConv(1024, 512) \n",
    "        self.UC2 = UpConv(512, 256) \n",
    "        self.UC3 = UpConv(256, 128) \n",
    "        self.UC4= UpConv(128, 64)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(64, 22, kernel_size = 1)\n",
    "        self.soft = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.TCB1(x)\n",
    "        x1 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB2(x)\n",
    "        x2 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB3(x)\n",
    "        x3 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB4(x)\n",
    "        x4 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB5(x)\n",
    "\n",
    "        x = self.UC1(x)\n",
    "        x = torch.cat([x4, x], dim = 1)\n",
    "        x = self.TCB6(x)\n",
    "\n",
    "        x = self.UC2(x)\n",
    "        x = torch.cat([x3, x], dim = 1)\n",
    "        x = self.TCB7(x)\n",
    "\n",
    "        x = self.UC3(x)\n",
    "        x = torch.cat([x2, x], dim = 1)\n",
    "        x = self.TCB8(x)\n",
    "\n",
    "        x = self.UC4(x)\n",
    "        x = torch.cat([x1, x], dim = 1)\n",
    "        x = self.TCB9(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        #x = self.soft(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = UNet_2D().to(device)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 損失誤差を計算\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        epoch_loss += loss.item()\n",
    "        # バックプロパゲーション\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    epoch_loss /= size\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            validation_loss += loss_fn(pred, y).item()\n",
    "            #print(pred.shape,y.shape)\n",
    "            _, tags = torch.max(pred, dim=1)\n",
    "            correct = (tags == y).float()\n",
    "            acc = correct.sum() / correct.numel()\n",
    "            #print(acc)\n",
    "            accs.append(acc.cpu().numpy())\n",
    "            #correct += (pred == y).type(torch.float).sum().item()\n",
    "    validation_loss /= size\n",
    "    accuracy = np.mean(accs)\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {validation_loss:>8f} \\n\")\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation損失をもとに最適epochを決定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.587212  [    0/ 1464]\n",
      "loss: 1.525828  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.099329 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.456708  [    0/ 1464]\n",
      "loss: 1.413468  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.096076 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.345865  [    0/ 1464]\n",
      "loss: 1.318661  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.094108 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.253337  [    0/ 1464]\n",
      "loss: 1.232892  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.092763 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.176751  [    0/ 1464]\n",
      "loss: 1.161616  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.091328 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.114864  [    0/ 1464]\n",
      "loss: 1.109739  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.091402 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.067191  [    0/ 1464]\n",
      "loss: 1.068070  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.089469 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.037738  [    0/ 1464]\n",
      "loss: 1.035828  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.089480 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.017233  [    0/ 1464]\n",
      "loss: 1.012763  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.090753 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.998930  [    0/ 1464]\n",
      "loss: 0.990936  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.090515 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.963667  [    0/ 1464]\n",
      "loss: 0.966382  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.088051 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.940111  [    0/ 1464]\n",
      "loss: 0.956515  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.090729 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.927610  [    0/ 1464]\n",
      "loss: 0.948496  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.088684 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.910894  [    0/ 1464]\n",
      "loss: 0.930407  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.090194 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.919695  [    0/ 1464]\n",
      "loss: 0.905222  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.090569 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.906704  [    0/ 1464]\n",
      "loss: 0.891706  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.088926 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.887475  [    0/ 1464]\n",
      "loss: 0.882947  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.089463 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.861063  [    0/ 1464]\n",
      "loss: 0.856049  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.088314 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.844296  [    0/ 1464]\n",
      "loss: 0.842625  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.088529 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.837904  [    0/ 1464]\n",
      "loss: 0.839321  [  800/ 1464]\n",
      "Validation Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.090684 \n",
      "\n",
      "11\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "best_epoch = 0\n",
    "min_loss = 100000\n",
    "os.makedirs(\"./model/\", exist_ok=True)\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    validation_loss = validation(validation_dataloader, model)\n",
    "    validation_losses.append(validation_loss)\n",
    "    if validation_loss < min_loss:\n",
    "        best_epoch = t+1\n",
    "        min_loss = validation_loss\n",
    "    torch.save(model.state_dict(), \"./model/model_\"+str(t+1)+\".pth\")\n",
    "print(best_epoch)\n",
    "\n",
    "#model = UNet_2D()\n",
    "#model.load_state_dict(torch.load(\"model/model_\"+str(best_epoch)+\".pth\"))\n",
    "#test(test_dataloader, model)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失グラフ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
